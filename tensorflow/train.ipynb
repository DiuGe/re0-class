{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Model loaded.\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import optimizers, applications\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "else:\n",
    "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "# get the pretrained neural network\n",
    "pretrained_conv = applications.VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "print('Pretrained Model loaded.')\n",
    "\n",
    "# pretrained_conv.summary()\n",
    "\n",
    "for layer in pretrained_conv.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the pretrained convolutional base model\n",
    "model.add(pretrained_conv)\n",
    "\n",
    "# First\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-4),\n",
    "              metrics=['categorical_accuracy'])\n",
    "print('Model compiled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                802848    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,827,298\n",
      "Trainable params: 5,522,530\n",
      "Non-trainable params: 15,304,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2406 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Reference:\n",
    "https://keras.io/preprocessing/image/\n",
    "https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
    "'''\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "IMAGE_DATA_DIR = 'data_face_training/'\n",
    "\n",
    "# this is the augmentation configuration we will use for training and validation\n",
    "image_datagen = ImageDataGenerator(\n",
    "        rescale=1./IMAGE_SIZE,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = image_datagen.flow_from_directory(\n",
    "        IMAGE_DATA_DIR,  # this is the target directory\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),  # all images will be resized to 150x150\n",
    "        batch_size=BATCH_SIZE,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        subset='training')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = image_datagen.flow_from_directory(\n",
    "        IMAGE_DATA_DIR,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'happy': 0, 'unhappy': 1}\n"
     ]
    }
   ],
   "source": [
    "label_map = (train_generator.class_indices)\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 493s 16s/step - loss: 0.7496 - categorical_accuracy: 0.5897 - val_loss: 0.5903 - val_categorical_accuracy: 0.7005\n",
      "Epoch 2/50\n",
      " 7/31 [=====>........................] - ETA: 5:04 - loss: 0.5524 - categorical_accuracy: 0.7366"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=1000 // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=400 // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_MODEL_DIR = 'output_models/'\n",
    "output_model = 'face-expression-model.h5'\n",
    "\n",
    "model.save(OUTPUT_MODEL_DIR+output_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
