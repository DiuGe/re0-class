{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Model loaded.\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import optimizers, applications\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "else:\n",
    "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "# get the pretrained neural network\n",
    "pretrained_conv = applications.VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "print('Pretrained Model loaded.')\n",
    "\n",
    "# pretrained_conv.summary()\n",
    "\n",
    "for layer in pretrained_conv.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the pretrained convolutional base model\n",
    "model.add(pretrained_conv)\n",
    "\n",
    "# First\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-4),\n",
    "              metrics=['categorical_accuracy'])\n",
    "print('Model compiled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                802848    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,827,298\n",
      "Trainable params: 5,522,530\n",
      "Non-trainable params: 15,304,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2406 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Reference:\n",
    "https://keras.io/preprocessing/image/\n",
    "https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
    "'''\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "IMAGE_DATA_DIR = 'data_face_training/'\n",
    "\n",
    "# this is the augmentation configuration we will use for training and validation\n",
    "image_datagen = ImageDataGenerator(\n",
    "        rescale=1./IMAGE_SIZE,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = image_datagen.flow_from_directory(\n",
    "        IMAGE_DATA_DIR,  # this is the target directory\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),  # all images will be resized to 150x150\n",
    "        batch_size=BATCH_SIZE,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        subset='training')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = image_datagen.flow_from_directory(\n",
    "        IMAGE_DATA_DIR,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'happy': 0, 'unhappy': 1}\n"
     ]
    }
   ],
   "source": [
    "label_map = (train_generator.class_indices)\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 46s 15s/step - loss: 0.3529 - categorical_accuracy: 0.8125 - val_loss: 0.4046 - val_categorical_accuracy: 0.8125\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 41s 14s/step - loss: 0.4025 - categorical_accuracy: 0.8750 - val_loss: 0.6721 - val_categorical_accuracy: 0.7812\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 41s 14s/step - loss: 0.3777 - categorical_accuracy: 0.8542 - val_loss: 0.3000 - val_categorical_accuracy: 0.9062\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 42s 14s/step - loss: 0.2914 - categorical_accuracy: 0.8958 - val_loss: 0.3433 - val_categorical_accuracy: 0.8438\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 41s 14s/step - loss: 0.4078 - categorical_accuracy: 0.8229 - val_loss: 0.2134 - val_categorical_accuracy: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x142ecc1d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorboard reference: https://www.tensorflow.org/tensorboard/get_started\n",
    "import datetime\n",
    "import keras\n",
    "%load_ext tensorboard\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100 // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=40 // BATCH_SIZE,\n",
    "        callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6dcf5121b9d66518\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6dcf5121b9d66518\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_MODEL_DIR = 'output_models/'\n",
    "output_model = 'face-expression-model.h5'\n",
    "\n",
    "model.save(OUTPUT_MODEL_DIR+output_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
